{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92673c53-33b7-4311-8759-c07f5f205067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "paper_source_directory = '/home/louis/research/pdf_processor/processed_data/superconductivity_processed/physrevb.71.134526'\n",
    "#paper_source_directory = '/home/louis/research/pdf_processor/processed_data/superconductivity_processed/physrevb.88.144511'\n",
    "paper_source_directory = '/home/louis/research/pdf_processor/processed_data/superconductivity_processed/physrevb.86.214518'\n",
    "file_name = 'text.txt'\n",
    "\n",
    "with open(os.path.join(paper_source_directory, file_name)) as f:\n",
    "    paper_text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3afd7ae2-3772-4c0d-b694-43ca098c40ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ea70545d53458b98171ff209d7f4d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "SYS_PROMPT = \"\"\"You are a graduate student research assistant in physics. \n",
    "You are given the extracted parts of a long document and a question. Read the document and don't make up an answer.\"\"\"\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# use quantization to lower GPU usage                                                \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8e904430-4987-40d2-9590-58541f46f254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(prompt, paper_text):\n",
    "  PROMPT = f\"Question: {prompt}\\nContext: \" + paper_text\n",
    "  return PROMPT\n",
    "    \\\n",
    "\n",
    "def generate(formatted_prompt):\n",
    "  formatted_prompt = formatted_prompt[:16000] # to avoid GPU OOM                      \n",
    "  messages = [{\"role\":\"system\",\"content\":SYS_PROMPT}, {\"role\":\"user\",\"content\":formatted_prompt}]\n",
    "  # tell the model to generate                                                       \n",
    "  input_ids = tokenizer.apply_chat_template(\n",
    "      messages,\n",
    "      add_generation_prompt=True,\n",
    "      return_tensors=\"pt\"\n",
    "  ).to(model.device)\n",
    "\n",
    "  print(input_ids)\n",
    "  print(input_ids.shape)\n",
    "  outputs = model.generate(\n",
    "      input_ids,\n",
    "      max_new_tokens=1024,\n",
    "      eos_token_id=terminators,\n",
    "      do_sample=True,\n",
    "      temperature=0.6,\n",
    "      top_p=0.9,\n",
    "  )\n",
    "  response = outputs[0][input_ids.shape[-1]:]\n",
    "  return tokenizer.decode(response, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b236041a-bdd0-478f-a8aa-69aa1ce5280b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 4334])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'MATERIAL: Sr0.9La0.1CuO2, Sr0.9Gd0.1CuO2, Gd.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(format_prompt(\"What is the material studied in this paper? Format the answer as MATERIAL: {Chemical Formula}. If there are multiple materials, separate them with commas.\", paper_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bdf70976-e90a-4e4a-82fc-1564a6c10070",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'CRITICAL TEMPERATURE: 43 K'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(format_prompt(\"What is the critical temperature at zero-field of the material studied in this paper? Just give a number and do not provide any explanation. The critical temperature is sometimes expressed as Tc, T_c, $T_c$, or $T_{c}$. Format the answer as CRITICAL TEMPERATURE: {Number} K. If there are multiple critical temperatures, separate them with commas.\", paper_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5bb6cf95-2ae5-4081-b8f1-62b41e59eba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'MAGNETIC FIELD: 160 T'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(format_prompt(\"What is upper critical field of the material studied in this paper? Format the answer as MAGNETIC FIELD: {Number} T\", paper_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f7d8d3-289f-470c-a566-653a45d3be69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
